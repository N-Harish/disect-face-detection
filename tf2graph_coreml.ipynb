{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93d0dd6-df8c-4dfd-b149-ddb63c02224d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b74870-94d0-41e9-bf61-39256a0bd8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 06:06:55.492479: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29ab730-8d60-4069-a832-96dd09e81127",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb791ff5-0146-4859-a6a7-6e599a9d3ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d451faa0-288c-486a-9caa-12ba7df2915a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # models.py\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Model\n",
    "# from tensorflow.keras.applications import MobileNetV2, ResNet50\n",
    "# from tensorflow.keras.layers import Input, Conv2D, ReLU, LeakyReLU\n",
    "# from modules.anchor import decode_tf, prior_box_tf\n",
    "\n",
    "\n",
    "# def _regularizer(weights_decay):\n",
    "#     \"\"\"l2 regularizer\"\"\"\n",
    "#     return tf.keras.regularizers.l2(weights_decay)\n",
    "\n",
    "\n",
    "# def _kernel_init(scale=1.0, seed=None):\n",
    "#     \"\"\"He normal initializer\"\"\"\n",
    "#     return tf.keras.initializers.he_normal()\n",
    "\n",
    "\n",
    "# class BatchNormalization(tf.keras.layers.BatchNormalization):\n",
    "#     \"\"\"Make trainable=False freeze BN for real (the og version is sad).\n",
    "#        ref: https://github.com/zzh8829/yolov3-tf2\n",
    "#     \"\"\"\n",
    "#     def __init__(self, axis=-1, momentum=0.9, epsilon=1e-5, center=True,\n",
    "#                  scale=True, name=None, **kwargs):\n",
    "#         super(BatchNormalization, self).__init__(\n",
    "#             axis=axis, momentum=momentum, epsilon=epsilon, center=center,\n",
    "#             scale=scale, name=name, **kwargs)\n",
    "\n",
    "#     def call(self, x, training=False):\n",
    "#         if training is None:\n",
    "#             training = tf.constant(False)\n",
    "#         training = tf.logical_and(training, self.trainable)\n",
    "\n",
    "#         return super().call(x, training)\n",
    "\n",
    "\n",
    "# def Backbone(backbone_type='ResNet50', use_pretrain=True):\n",
    "#     \"\"\"Backbone Model\"\"\"\n",
    "#     weights = None\n",
    "#     if use_pretrain:\n",
    "#         weights = 'imagenet'\n",
    "\n",
    "#     def backbone(x):\n",
    "#         if backbone_type == 'ResNet50':\n",
    "#             extractor = ResNet50(\n",
    "#                 input_shape=x.shape[1:], include_top=False, weights=weights)\n",
    "#             pick_layer1 = 80  # [80, 80, 512]\n",
    "#             pick_layer2 = 142  # [40, 40, 1024]\n",
    "#             pick_layer3 = 174  # [20, 20, 2048]\n",
    "#             preprocess = tf.keras.applications.resnet.preprocess_input\n",
    "#         elif backbone_type == 'MobileNetV2':\n",
    "#             extractor = MobileNetV2(\n",
    "#                 input_shape=x.shape[1:], include_top=False, weights=weights)\n",
    "#             pick_layer1 = 54  # [80, 80, 32]\n",
    "#             pick_layer2 = 116  # [40, 40, 96]\n",
    "#             pick_layer3 = 143  # [20, 20, 160]\n",
    "#             preprocess = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "#         else:\n",
    "#             raise NotImplementedError(\n",
    "#                 'Backbone type {} is not recognized.'.format(backbone_type))\n",
    "\n",
    "#         return Model(extractor.input,\n",
    "#                      (extractor.layers[pick_layer1].output,\n",
    "#                       extractor.layers[pick_layer2].output,\n",
    "#                       extractor.layers[pick_layer3].output),\n",
    "#                      name=backbone_type + '_extrator')(preprocess(x))\n",
    "\n",
    "#     return backbone\n",
    "\n",
    "\n",
    "# class ConvUnit(tf.keras.layers.Layer):\n",
    "#     \"\"\"Conv + BN + Act\"\"\"\n",
    "#     def __init__(self, f, k, s, wd, act=None, name='ConvBN', **kwargs):\n",
    "#         super(ConvUnit, self).__init__(name=name, **kwargs)\n",
    "#         self.conv = Conv2D(filters=f, kernel_size=k, strides=s, padding='same',\n",
    "#                            kernel_initializer=_kernel_init(),\n",
    "#                            kernel_regularizer=_regularizer(wd),\n",
    "#                            use_bias=False, name='conv')\n",
    "#         self.bn = BatchNormalization(name='bn')\n",
    "\n",
    "#         if act is None:\n",
    "#             self.act_fn = tf.identity\n",
    "#         elif act == 'relu':\n",
    "#             self.act_fn = ReLU()\n",
    "#         elif act == 'lrelu':\n",
    "#             self.act_fn = LeakyReLU(0.1)\n",
    "#         else:\n",
    "#             raise NotImplementedError(\n",
    "#                 'Activation function type {} is not recognized.'.format(act))\n",
    "\n",
    "#     def call(self, x):\n",
    "#         return self.act_fn(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "# class FPN(tf.keras.layers.Layer):\n",
    "#     \"\"\"Feature Pyramid Network\"\"\"\n",
    "\n",
    "#     def __init__(self, out_ch, wd, name='FPN', **kwargs):\n",
    "#         super(FPN, self).__init__(name=name, **kwargs)\n",
    "#         act = 'relu'\n",
    "#         if (out_ch <= 64):\n",
    "#             act = 'lrelu'\n",
    "\n",
    "#         self.output1 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=act)\n",
    "#         self.output2 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=act)\n",
    "#         self.output3 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=act)\n",
    "#         self.merge1 = ConvUnit(f=out_ch, k=3, s=1, wd=wd, act=act)\n",
    "#         self.merge2 = ConvUnit(f=out_ch, k=3, s=1, wd=wd, act=act)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         output1 = self.output1(x[0])  # [80, 80, out_ch]\n",
    "#         output2 = self.output2(x[1])  # [40, 40, out_ch]\n",
    "#         output3 = self.output3(x[2])  # [20, 20, out_ch]\n",
    "\n",
    "#         up_h, up_w = tf.shape(output2)[1], tf.shape(output2)[2]\n",
    "#         up3 = tf.image.resize(output3, [up_h, up_w], method='nearest')\n",
    "#         output2 = output2 + up3\n",
    "#         output2 = self.merge2(output2)\n",
    "\n",
    "#         up_h, up_w = tf.shape(output1)[1], tf.shape(output1)[2]\n",
    "#         up2 = tf.image.resize(output2, [up_h, up_w], method='nearest')\n",
    "#         output1 = output1 + up2\n",
    "#         output1 = self.merge1(output1)\n",
    "\n",
    "#         return output1, output2, output3\n",
    "\n",
    "\n",
    "# class SSH(tf.keras.layers.Layer):\n",
    "#     \"\"\"Single Stage Headless Layer\"\"\"\n",
    "#     def __init__(self, out_ch, wd, name='SSH', **kwargs):\n",
    "#         super(SSH, self).__init__(name=name, **kwargs)\n",
    "#         assert out_ch % 4 == 0\n",
    "#         act = 'relu'\n",
    "#         if (out_ch <= 64):\n",
    "#             act = 'lrelu'\n",
    "\n",
    "#         self.conv_3x3 = ConvUnit(f=out_ch // 2, k=3, s=1, wd=wd, act=None)\n",
    "\n",
    "#         self.conv_5x5_1 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=act)\n",
    "#         self.conv_5x5_2 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=None)\n",
    "\n",
    "#         self.conv_7x7_2 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=act)\n",
    "#         self.conv_7x7_3 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=None)\n",
    "\n",
    "#         self.relu = ReLU()\n",
    "\n",
    "#     def call(self, x):\n",
    "#         conv_3x3 = self.conv_3x3(x)\n",
    "\n",
    "#         conv_5x5_1 = self.conv_5x5_1(x)\n",
    "#         conv_5x5 = self.conv_5x5_2(conv_5x5_1)\n",
    "\n",
    "#         conv_7x7_2 = self.conv_7x7_2(conv_5x5_1)\n",
    "#         conv_7x7 = self.conv_7x7_3(conv_7x7_2)\n",
    "\n",
    "#         output = tf.concat([conv_3x3, conv_5x5, conv_7x7], axis=3)\n",
    "#         output = self.relu(output)\n",
    "\n",
    "#         return output\n",
    "\n",
    "\n",
    "# class BboxHead(tf.keras.layers.Layer):\n",
    "#     \"\"\"Bbox Head Layer\"\"\"\n",
    "#     def __init__(self, num_anchor, wd, name='BboxHead', **kwargs):\n",
    "#         super(BboxHead, self).__init__(name=name, **kwargs)\n",
    "#         self.num_anchor = num_anchor\n",
    "#         self.conv = Conv2D(filters=num_anchor * 4, kernel_size=1, strides=1)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         h, w = tf.shape(x)[1], tf.shape(x)[2]\n",
    "#         x = self.conv(x)\n",
    "\n",
    "#         return tf.reshape(x, [-1, h * w * self.num_anchor, 4])\n",
    "\n",
    "\n",
    "# class LandmarkHead(tf.keras.layers.Layer):\n",
    "#     \"\"\"Landmark Head Layer\"\"\"\n",
    "#     def __init__(self, num_anchor, wd, name='LandmarkHead', **kwargs):\n",
    "#         super(LandmarkHead, self).__init__(name=name, **kwargs)\n",
    "#         self.num_anchor = num_anchor\n",
    "#         self.conv = Conv2D(filters=num_anchor * 10, kernel_size=1, strides=1)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         h, w = tf.shape(x)[1], tf.shape(x)[2]\n",
    "#         x = self.conv(x)\n",
    "\n",
    "#         return tf.reshape(x, [-1, h * w * self.num_anchor, 10])\n",
    "\n",
    "\n",
    "# class ClassHead(tf.keras.layers.Layer):\n",
    "#     \"\"\"Class Head Layer\"\"\"\n",
    "#     def __init__(self, num_anchor, wd, name='ClassHead', **kwargs):\n",
    "#         super(ClassHead, self).__init__(name=name, **kwargs)\n",
    "#         self.num_anchor = num_anchor\n",
    "#         self.conv = Conv2D(filters=num_anchor * 2, kernel_size=1, strides=1)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         h, w = tf.shape(x)[1], tf.shape(x)[2]\n",
    "#         x = self.conv(x)\n",
    "\n",
    "#         # print('*'*100)\n",
    "#         # print(tf.shape(x))\n",
    "#         return tf.reshape(x, [-1, h * w * self.num_anchor, 2])\n",
    "\n",
    "\n",
    "# def RetinaFaceModel(cfg, training=False, iou_th=0.4, score_th=0.02,\n",
    "#                     name='RetinaFaceModel'):\n",
    "#     \"\"\"Retina Face Model\"\"\"\n",
    "#     tf.keras.backend.set_image_data_format('channels_last')\n",
    "#     input_size = cfg['input_size'] if training else None\n",
    "#     wd = cfg['weights_decay']\n",
    "#     out_ch = cfg['out_channel']\n",
    "#     num_anchor = len(cfg['min_sizes'][0])\n",
    "#     backbone_type = cfg['backbone_type']\n",
    "\n",
    "#     # define model\n",
    "#     x = inputs = Input([input_size, input_size, 3], name='input_image')\n",
    "\n",
    "#     x = Backbone(backbone_type=backbone_type)(x)\n",
    "\n",
    "#     fpn = FPN(out_ch=out_ch, wd=wd)(x)\n",
    "    \n",
    "#     out = fpn\n",
    "# #     features = [SSH(out_ch=out_ch, wd=wd, name=f'SSH_{i}')(f)\n",
    "# #                 for i, f in enumerate(fpn)]\n",
    "\n",
    "# #     bbox_regressions = tf.concat(\n",
    "# #         [BboxHead(num_anchor, wd=wd, name=f'BboxHead_{i}')(f)\n",
    "# #          for i, f in enumerate(features)], axis=1)\n",
    "# #     # landm_regressions = tf.concat(\n",
    "# #     #     [LandmarkHead(num_anchor, wd=wd, name=f'LandmarkHead_{i}')(f)\n",
    "# #     #      for i, f in enumerate(features)], axis=1)\n",
    "# #     classifications = tf.concat(\n",
    "# #         [ClassHead(num_anchor, wd=wd, name=f'ClassHead_{i}')(f)\n",
    "# #          for i, f in enumerate(features)], axis=1)\n",
    "\n",
    "# #     classifications = tf.keras.layers.Softmax(axis=-1)(classifications)\n",
    "\n",
    "# #     if training:\n",
    "# #         out = (bbox_regressions, \n",
    "# #                # landm_regressions, \n",
    "# #                classifications)\n",
    "# #     else:\n",
    "# #         out = (bbox_regressions, \n",
    "# #                # landm_regressions, \n",
    "# #                classifications)\n",
    "\n",
    "\n",
    "# #         # only for batch size 1\n",
    "# #         preds = tf.concat(  # [bboxes, landms, landms_valid, conf]\n",
    "# #             [bbox_regressions[0], \n",
    "# #              # landm_regressions[0],\n",
    "# #              # tf.ones_like(classifications[0, :, 0][..., tf.newaxis]),\n",
    "# #              classifications[0, :, 1][..., tf.newaxis]\n",
    "# #             ], 1)\n",
    "# #         priors = prior_box_tf((tf.shape(inputs)[1], tf.shape(inputs)[2]),\n",
    "# #                               cfg['min_sizes'],  cfg['steps'], cfg['clip'])\n",
    "# #         decode_preds = decode_tf(preds, priors, cfg['variances'])\n",
    "\n",
    "# #         selected_indices = tf.image.non_max_suppression(\n",
    "# #             boxes=decode_preds[:, :4],\n",
    "# #             scores=decode_preds[:, -1],\n",
    "# #             max_output_size=tf.shape(decode_preds)[0],\n",
    "# #             iou_threshold=iou_th,\n",
    "# #             score_threshold=score_th)\n",
    "\n",
    "# #         out = tf.gather(decode_preds, selected_indices)\n",
    "\n",
    "#     return Model(inputs, out, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be98425-00d0-48a9-ba65-667fc49e1647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, LeakyReLU\n",
    "from modules.anchor import decode_tf, prior_box_tf\n",
    "\n",
    "\n",
    "def _regularizer(weights_decay):\n",
    "    \"\"\"l2 regularizer\"\"\"\n",
    "    return tf.keras.regularizers.l2(weights_decay)\n",
    "\n",
    "\n",
    "def _kernel_init(scale=1.0, seed=None):\n",
    "    \"\"\"He normal initializer\"\"\"\n",
    "    return tf.keras.initializers.he_normal()\n",
    "\n",
    "\n",
    "class BatchNormalization(tf.keras.layers.BatchNormalization):\n",
    "    \"\"\"Make trainable=False freeze BN for real (the og version is sad).\n",
    "       ref: https://github.com/zzh8829/yolov3-tf2\n",
    "    \"\"\"\n",
    "    def __init__(self, axis=-1, momentum=0.9, epsilon=1e-5, center=True,\n",
    "                 scale=True, name=None, **kwargs):\n",
    "        super(BatchNormalization, self).__init__(\n",
    "            axis=axis, momentum=momentum, epsilon=epsilon, center=center,\n",
    "            scale=scale, name=name, **kwargs)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if training is None:\n",
    "            training = tf.constant(False)\n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "\n",
    "        return super().call(x, training)\n",
    "\n",
    "\n",
    "def Backbone(backbone_type='ResNet50', use_pretrain=True):\n",
    "    \"\"\"Backbone Model\"\"\"\n",
    "    weights = None\n",
    "    if use_pretrain:\n",
    "        weights = 'imagenet'\n",
    "\n",
    "    def backbone(x):\n",
    "        if backbone_type == 'ResNet50':\n",
    "            extractor = ResNet50(\n",
    "                input_shape=x.shape[1:], include_top=False, weights=weights)\n",
    "            pick_layer1 = 80  # [80, 80, 512]\n",
    "            pick_layer2 = 142  # [40, 40, 1024]\n",
    "            pick_layer3 = 174  # [20, 20, 2048]\n",
    "            preprocess = tf.keras.applications.resnet.preprocess_input\n",
    "        elif backbone_type == 'MobileNetV2':\n",
    "            extractor = MobileNetV2(\n",
    "                input_shape=x.shape[1:], include_top=False, weights=weights)\n",
    "            pick_layer1 = 54  # [80, 80, 32]\n",
    "            pick_layer2 = 116  # [40, 40, 96]\n",
    "            pick_layer3 = 143  # [20, 20, 160]\n",
    "            preprocess = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'Backbone type {} is not recognized.'.format(backbone_type))\n",
    "\n",
    "        return Model(extractor.input,\n",
    "                     (extractor.layers[pick_layer1].output,\n",
    "                      extractor.layers[pick_layer2].output,\n",
    "                      extractor.layers[pick_layer3].output),\n",
    "                     name=backbone_type + '_extrator')(preprocess(x))\n",
    "\n",
    "    return backbone\n",
    "\n",
    "\n",
    "class ConvUnit(tf.keras.layers.Layer):\n",
    "    \"\"\"Conv + BN + Act\"\"\"\n",
    "    def __init__(self, f=None, k=None, s=None, wd=None, act=None, name='ConvBN', **kwargs):\n",
    "        super(ConvUnit, self).__init__(name=name, **kwargs)\n",
    "        self.f = f\n",
    "        self.k = k\n",
    "        self.s = s\n",
    "        self.wd = wd\n",
    "        self.act = act\n",
    "#         self.name = name\n",
    "        self.conv = Conv2D(filters=f, kernel_size=k, strides=s, padding='same',\n",
    "                           kernel_initializer=_kernel_init(),\n",
    "                           kernel_regularizer=_regularizer(wd),\n",
    "                           use_bias=False, name='conv')\n",
    "\n",
    "        # self.bn = BatchNormalization(name='bn')\n",
    "        self.bn = tf.keras.layers.BatchNormalization(name='bn')\n",
    "\n",
    "        if act is None:\n",
    "            self.act_fn = tf.identity\n",
    "        elif act == 'relu':\n",
    "            self.act_fn = ReLU()\n",
    "        elif act == 'lrelu':\n",
    "            self.act_fn = LeakyReLU(0.1)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'Activation function type {} is not recognized.'.format(act))\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.act_fn(self.bn(self.conv(x)))\n",
    "        # return self.bn(self.conv(x))\n",
    "        # return self.act_fn(self.conv(x))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        temp = {\n",
    "            'f': self.f,\n",
    "            'k': self.k,\n",
    "            's': self.s,\n",
    "            'wd': self.wd,\n",
    "            'act': self.act,\n",
    "#             'convunit_name': self.name,\n",
    "#             'conv': self.conv,\n",
    "#             'bn': self.bn,\n",
    "#             'act_fn': self.act_fn,\n",
    "        }\n",
    "        config.update(temp)\n",
    "        return config\n",
    "\n",
    "\n",
    "class FPN(tf.keras.layers.Layer):\n",
    "    \"\"\"Feature Pyramid Network\"\"\"\n",
    "    def __init__(self, out_ch, wd, name='FPN', **kwargs):\n",
    "        super(FPN, self).__init__(name=name, **kwargs)\n",
    "        self.act = 'relu'\n",
    "        self.out_ch = out_ch\n",
    "#         self.name = name\n",
    "        self.wd = wd\n",
    "        if (out_ch <= 64):\n",
    "            self.act = 'lrelu'\n",
    "\n",
    "        self.output1 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=self.act, name=name+str(1))\n",
    "        self.output2 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=self.act, name=name+str(2))\n",
    "        self.output3 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=self.act, name=name+str(3))\n",
    "        self.merge1 = ConvUnit(f=out_ch, k=3, s=1, wd=wd, act=self.act, name=name+str(4))\n",
    "        self.merge2 = ConvUnit(f=out_ch, k=3, s=1, wd=wd, act=self.act, name=name+str(5))\n",
    "\n",
    "    def call(self, x):\n",
    "        print(f\"x[0] :- {x[0].shape}\")\n",
    "        print(f\"x[1] :- {x[1].shape}\")\n",
    "        print(f\"x[2] :- {x[2].shape}\")\n",
    "        \n",
    "        print(f\"Running output1 conv layer\")\n",
    "        output1 = self.output1(x[0])  # [80, 80, out_ch]\n",
    "        print(f\"Output of output1 conv :- {output1.shape} \\n\")\n",
    "        \n",
    "        print(f\"Running output2 conv layer\")\n",
    "        output2 = self.output2(x[1])  # [40, 40, out_ch]\n",
    "        print(f\"Output of output2 conv :- {output2.shape} \\n\")\n",
    "        \n",
    "        print(f\"Running output3 conv layer\")\n",
    "        output3 = self.output3(x[2])  # [20, 20, out_ch]\n",
    "        print(f\"Output of output3 conv :- {output3.shape} \\n\")\n",
    "\n",
    "        print(f\"Running resize of output3 \\n\")\n",
    "        up_h, up_w = tf.shape(output2)[1], tf.shape(output2)[2]\n",
    "        \n",
    "        # up_h, up_w = output2.shape[1], output2.shape[2]\n",
    "        print(output2.shape[1], output2.shape[2])\n",
    "        \n",
    "        up3 = tf.image.resize(output3, [up_h, up_w], method='nearest')\n",
    "        print(f\"Resized output3 to :- {tf.shape(up3)} {up3.shape}\\n\")\n",
    "        \n",
    "        print(f\"Adding output2 to resized output3 \\n\")\n",
    "        # print(f\"Shape of tf.concat output2 ,up3 :- {tf.add(output2, up3).shape}\")\n",
    "        \n",
    "        # output2 = output2 + up3\n",
    "        output2 = tf.add(output2, up3)\n",
    "        print(f\"Output2 shape :- {tf.shape(output2)} {output2.shape}\\n\")\n",
    "        \n",
    "        print(f\"passing output2 to merge2 conv \\n\")\n",
    "        output2 = self.merge2(output2)\n",
    "        print(f\"Output of output2 :- {output2.shape} \\n\")\n",
    "        \n",
    "        print(f\"Running resize of output2 w.r.t. output1 \\n\")\n",
    "        up_h, up_w = tf.shape(output1)[1], tf.shape(output1)[2]\n",
    "        up2 = tf.image.resize(output2, [up_h, up_w], method='nearest')\n",
    "        print(f\"Resized output2 to :- {tf.shape(up2)} {up2.shape}\\n\")\n",
    "        \n",
    "        print(f\"Adding output1 to resized output2 \\n\")\n",
    "        \n",
    "        # output1 = output1 + up2\n",
    "        output1 = tf.add(output1, up2)\n",
    "        print(f\"Output1 shape :- {tf.shape(output1)} {output1.shape}\\n\")\n",
    "        \n",
    "        print(f\"Running merge1 conv on output1 \\n\")\n",
    "        output1 = self.merge1(output1)\n",
    "        print(f\"Shape of output1 :- {output1.shape} \\n\")\n",
    "\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        temp = {\n",
    "#             'fpn_act': self.act,\n",
    "            'out_ch': self.out_ch,\n",
    "#             'fpn_name': self.name,\n",
    "            'wd': self.wd,\n",
    "#             'fpn_conv1': self.output1,\n",
    "#             'fpn_conv2': self.output2,\n",
    "#             'fpn_conv3': self.output3,\n",
    "#             'fpn_merge1': self.merge1,\n",
    "#             'fpn_merge2': self.merge2,\n",
    "        }\n",
    "        config.update(temp)\n",
    "        return config\n",
    "\n",
    "\n",
    "class SSH(tf.keras.layers.Layer):\n",
    "    \"\"\"Single Stage Headless Layer\"\"\"\n",
    "    def __init__(self, out_ch, wd, name='SSH', **kwargs):\n",
    "        super(SSH, self).__init__(name=name, **kwargs)\n",
    "        assert out_ch % 4 == 0\n",
    "        self.out_ch = out_ch\n",
    "#         self.name = name\n",
    "        self.wd = wd\n",
    "        self.act = 'relu'\n",
    "        if (out_ch <= 64):\n",
    "            self.act = 'lrelu'\n",
    "\n",
    "        self.conv_3x3 = ConvUnit(f=out_ch // 2, k=3, s=1, wd=wd, act=None, name=name+str(1))\n",
    "\n",
    "        self.conv_5x5_1 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=self.act, name=name+str(2))\n",
    "        self.conv_5x5_2 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=None, name=name+str(3))\n",
    "\n",
    "        self.conv_7x7_2 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=self.act, name=name+str(4))\n",
    "        self.conv_7x7_3 = ConvUnit(f=out_ch // 4, k=3, s=1, wd=wd, act=None, name=name+str(5))\n",
    "\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        conv_3x3 = self.conv_3x3(x)\n",
    "\n",
    "        conv_5x5_1 = self.conv_5x5_1(x)\n",
    "        conv_5x5 = self.conv_5x5_2(conv_5x5_1)\n",
    "\n",
    "        conv_7x7_2 = self.conv_7x7_2(conv_5x5_1)\n",
    "        conv_7x7 = self.conv_7x7_3(conv_7x7_2)\n",
    "\n",
    "        output = tf.concat([conv_3x3, conv_5x5, conv_7x7], axis=3)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        temp = {\n",
    "            'out_ch': self.out_ch,\n",
    "#             'ssh_name': self.name,\n",
    "            'wd': self.wd,\n",
    "#             'ssh_act': self.act,\n",
    "#             'ssh_conv_3x3': self.conv_3x3,\n",
    "#             'ssh_conv_5x5_1': self.conv_5x5_1,\n",
    "#             'ssh_conv_5x5_2': self.conv_5x5_2,\n",
    "#             'ssh_conv_7x7_2': self.conv_7x7_2,\n",
    "#             'ssh_conv_7x7_3': self.conv_7x7_3,\n",
    "#             'ssh_relu': self.relu,\n",
    "        }\n",
    "        config.update(temp)\n",
    "        return config\n",
    "\n",
    "\n",
    "class BboxHead(tf.keras.layers.Layer):\n",
    "    \"\"\"Bbox Head Layer\"\"\"\n",
    "    def __init__(self, num_anchor, wd, name='BboxHead', **kwargs):\n",
    "        super(BboxHead, self).__init__(name=name, **kwargs)\n",
    "#         self.name = name\n",
    "        self.wd = wd\n",
    "        self.num_anchor = num_anchor\n",
    "        self.conv = Conv2D(filters=num_anchor * 4, kernel_size=1, strides=1, name=name)\n",
    "\n",
    "    def call(self, x):\n",
    "        h, w = tf.shape(x)[1], tf.shape(x)[2]\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return tf.reshape(x, [-1, h * w * self.num_anchor, 4])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        temp = {\n",
    "#             'bboxhead_name': self.name,\n",
    "            'wd': self.wd,\n",
    "            'num_anchor': self.num_anchor,\n",
    "#             'bboxhead_conv': self.conv,\n",
    "        }\n",
    "        config.update(temp)\n",
    "        return config\n",
    "\n",
    "\n",
    "class LandmarkHead(tf.keras.layers.Layer):\n",
    "    \"\"\"Landmark Head Layer\"\"\"\n",
    "    def __init__(self, num_anchor, wd, name='LandmarkHead', **kwargs):\n",
    "        super(LandmarkHead, self).__init__(name=name, **kwargs)\n",
    "        self.num_anchor = num_anchor\n",
    "#         self.name = name\n",
    "        self.wd = wd\n",
    "        self.conv = Conv2D(filters=num_anchor * 10, kernel_size=1, strides=1, name=name)\n",
    "\n",
    "    def call(self, x):\n",
    "        h, w = tf.shape(x)[1], tf.shape(x)[2]\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return tf.reshape(x, [-1, h * w * self.num_anchor, 10])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        temp = {\n",
    "#             'landmarkhead_name': self.name,\n",
    "            'wd': self.wd,\n",
    "            'num_anchor': self.num_anchor,\n",
    "#             'landmarkhead_conv': self.conv,\n",
    "        }\n",
    "        config.update(temp)\n",
    "        return config\n",
    "\n",
    "\n",
    "class ClassHead(tf.keras.layers.Layer):\n",
    "    \"\"\"Class Head Layer\"\"\"\n",
    "    def __init__(self, num_anchor, wd, name='ClassHead', **kwargs):\n",
    "        super(ClassHead, self).__init__(name=name, **kwargs)\n",
    "#         self.name = name\n",
    "        self.wd = wd\n",
    "        self.num_anchor = num_anchor\n",
    "        self.conv = Conv2D(filters=num_anchor * 2, kernel_size=1, strides=1, name=name)\n",
    "\n",
    "    def call(self, x):\n",
    "        h, w = tf.shape(x)[1], tf.shape(x)[2]\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # print('*'*100)\n",
    "        # print(tf.shape(x))\n",
    "        return tf.reshape(x, [-1, h * w * self.num_anchor, 2])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        temp = {\n",
    "#             'classhead_name': self.name,\n",
    "            'wd': self.wd,\n",
    "            'num_anchor': self.num_anchor,\n",
    "#             'classhead_conv': self.conv,\n",
    "        }\n",
    "        config.update(temp)\n",
    "        return config\n",
    "\n",
    "\n",
    "def RetinaFaceModel(cfg, training=False, iou_th=0.4, score_th=0.02,\n",
    "                    name='RetinaFaceModel'):\n",
    "    \"\"\"Retina Face Model\"\"\"\n",
    "    tf.keras.backend.set_image_data_format('channels_last')\n",
    "    input_size = cfg['input_size'] if training else None\n",
    "    wd = cfg['weights_decay']\n",
    "    out_ch = cfg['out_channel']\n",
    "    print(out_ch)\n",
    "    \n",
    "    num_anchor = len(cfg['min_sizes'][0])\n",
    "    backbone_type = cfg['backbone_type']\n",
    "\n",
    "    # define model\n",
    "    x = inputs = Input([input_size, input_size, 3], name='input_image')\n",
    "\n",
    "    x = Backbone(backbone_type=backbone_type)(x)\n",
    "\n",
    "    fpn = FPN(out_ch=out_ch, wd=wd)(x)\n",
    "\n",
    "    # out = fpn\n",
    "\n",
    "    features = [SSH(out_ch=out_ch, wd=wd, name=f'SSH_{i}')(f)\n",
    "                for i, f in enumerate(fpn)]\n",
    "\n",
    "    bbox_regressions = tf.concat(\n",
    "        [BboxHead(num_anchor, wd=wd, name=f'BboxHead_{i}')(f)\n",
    "         for i, f in enumerate(features)], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # landm_regressions = tf.concat(\n",
    "    #     [LandmarkHead(num_anchor, wd=wd, name=f'LandmarkHead_{i}')(f)\n",
    "    #      for i, f in enumerate(features)], axis=1)\n",
    "    classifications = tf.concat(\n",
    "        [ClassHead(num_anchor, wd=wd, name=f'ClassHead_{i}')(f)\n",
    "         for i, f in enumerate(features)], axis=1)\n",
    "\n",
    "    classifications = tf.keras.layers.Softmax(axis=-1)(classifications)\n",
    "    \n",
    "    \n",
    "    bbox_reg = [BboxHead(num_anchor, wd=wd, name=f'BboxHead_{i}')(f)\n",
    "         for i, f in enumerate(features)]\n",
    "    \n",
    "    \n",
    "    clf = [ClassHead(num_anchor, wd=wd, name=f'ClassHead_{i}')(f)\n",
    "         for i, f in enumerate(features)]\n",
    "    \n",
    "    if training:\n",
    "        out = (bbox_regressions, \n",
    "               # landm_regressions, \n",
    "               classifications)\n",
    "        # out = (bbox_reg, clf)\n",
    "    else:\n",
    "        out = (bbox_regressions, \n",
    "               # landm_regressions, \n",
    "               classifications)\n",
    "        # out = (bbox_reg, clf)\n",
    "\n",
    "        \n",
    "#         # only for batch size 1\n",
    "#         preds = tf.concat(  # [bboxes, landms, landms_valid, conf]\n",
    "#             [bbox_regressions[0], \n",
    "#              # landm_regressions[0],\n",
    "#              # tf.ones_like(classifications[0, :, 0][..., tf.newaxis]),\n",
    "#              classifications[0, :, 1][..., tf.newaxis]\n",
    "#             ], 1)\n",
    "#         priors = prior_box_tf((tf.shape(inputs)[1], tf.shape(inputs)[2]),\n",
    "#                               cfg['min_sizes'],  cfg['steps'], cfg['clip'])\n",
    "#         decode_preds = decode_tf(preds, priors, cfg['variances'])\n",
    "\n",
    "#         selected_indices = tf.image.non_max_suppression(\n",
    "#             boxes=decode_preds[:, :4],\n",
    "#             scores=decode_preds[:, -1],\n",
    "#             max_output_size=tf.shape(decode_preds)[0],\n",
    "#             iou_threshold=iou_th,\n",
    "#             score_threshold=score_th)\n",
    "\n",
    "#         out = tf.gather(decode_preds, selected_indices)\n",
    "\n",
    "    return Model(inputs, out, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f8fbac-976e-45aa-9629-6e55d3211c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tf2onnx\n",
    "# import onnxruntime as rt\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python.keras.models import load_model\n",
    "import os\n",
    "# from modules.models import RetinaFaceModel\n",
    "from modules.utils import (set_memory_growth, load_yaml, load_dataset,\n",
    "                           ProgressBar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "830809c5-bd83-46bc-9953-c8c98e73377c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_path = './configs/retinaface_mbv2.yaml'\n",
    "cfg = load_yaml(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e348b3-0f40-45c2-9c4d-84b574760ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 06:06:59.959436: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-07-13 06:06:59.984212: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-07-13 06:06:59.984243: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ip-172-16-165-11.ec2.internal\n",
      "2023-07-13 06:06:59.984250: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ip-172-16-165-11.ec2.internal\n",
      "2023-07-13 06:06:59.984334: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.85.12\n",
      "2023-07-13 06:06:59.984356: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.85.12\n",
      "2023-07-13 06:06:59.984362: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.85.12\n",
      "2023-07-13 06:06:59.984618: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0] :- (None, None, None, 192)\n",
      "x[1] :- (None, None, None, 576)\n",
      "x[2] :- (None, None, None, 960)\n",
      "Running output1 conv layer\n",
      "Output of output1 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output2 conv layer\n",
      "Output of output2 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output3 conv layer\n",
      "Output of output3 conv :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output3 \n",
      "\n",
      "None None\n",
      "Resized output3 to :- Tensor(\"FPN/Shape_2:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output2 to resized output3 \n",
      "\n",
      "Output2 shape :- Tensor(\"FPN/Shape_3:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "passing output2 to merge2 conv \n",
      "\n",
      "Output of output2 :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output2 w.r.t. output1 \n",
      "\n",
      "Resized output2 to :- Tensor(\"FPN/Shape_6:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output1 to resized output2 \n",
      "\n",
      "Output1 shape :- Tensor(\"FPN/Shape_7:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Running merge1 conv on output1 \n",
      "\n",
      "Shape of output1 :- (None, None, None, 64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = RetinaFaceModel(cfg, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8fd6441-2ecc-44a7-8698-f460d4380305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# model.predict(np.random.rand(1, 800, 800, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383e042-5c50-4023-8575-40e90128b013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3453afce-e99c-406d-8cf3-0bbecbbd8523",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RetinaFaceModel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (None, None, None, 3 0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, None, None, 3 0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MobileNetV2_extrator (Functiona ((None, None, None,  1518464     tf.math.subtract[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FPN (FPN)                       ((None, None, None,  185600      MobileNetV2_extrator[0][0]       \n",
      "                                                                 MobileNetV2_extrator[0][1]       \n",
      "                                                                 MobileNetV2_extrator[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "SSH_0 (SSH)                     (None, None, None, 6 34944       FPN[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "SSH_1 (SSH)                     (None, None, None, 6 34944       FPN[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "SSH_2 (SSH)                     (None, None, None, 6 34944       FPN[0][2]                        \n",
      "__________________________________________________________________________________________________\n",
      "ClassHead_0 (ClassHead)         (None, None, 2)      260         SSH_0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ClassHead_1 (ClassHead)         (None, None, 2)      260         SSH_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ClassHead_2 (ClassHead)         (None, None, 2)      260         SSH_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BboxHead_0 (BboxHead)           (None, None, 4)      520         SSH_0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BboxHead_1 (BboxHead)           (None, None, 4)      520         SSH_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BboxHead_2 (BboxHead)           (None, None, 4)      520         SSH_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (None, None, 2)      0           ClassHead_0[0][0]                \n",
      "                                                                 ClassHead_1[0][0]                \n",
      "                                                                 ClassHead_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, None, 4)      0           BboxHead_0[0][0]                 \n",
      "                                                                 BboxHead_1[0][0]                 \n",
      "                                                                 BboxHead_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, None, 2)      0           tf.concat_1[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,811,236\n",
      "Trainable params: 1,782,948\n",
      "Non-trainable params: 28,288\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d3b1b9-c572-4008-9c42-8f49ac6283c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "865b7efc-faf2-4dbb-aae6-65c0f71f4327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fde5279-ceae-453e-8f26-40cff7184ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(\n",
    "#     model,\n",
    "#     show_shapes=False,\n",
    "#     show_dtype=False,\n",
    "#     show_layer_names=True,\n",
    "#     expand_nested=False,\n",
    "#     dpi=96\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d247de35-4742-4540-99ae-2c47ff87d562",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa0f81b3970>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint_dir = '/home/ec2-user/SageMaker/gaurav/retinaface-tf2_unlabelled_data_without_landmarks/all_checkpoints/checkpoint_1000'\n",
    "checkpoint_dir = './ScratchML-Models/Models/16may2023/face_detection/portrait/checkpoint_78/'\n",
    "checkpoint_dir = './ScratchML-Models/Models/16may2023/face_detection/landscape/checkpoint_2030/'\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b162bdea-b897-4a93-a1a8-88b574e0f86f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RetinaFaceModel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (None, None, None, 3 0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, None, None, 3 0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MobileNetV2_extrator (Functiona ((None, None, None,  1518464     tf.math.subtract[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "FPN (FPN)                       ((None, None, None,  185600      MobileNetV2_extrator[0][0]       \n",
      "                                                                 MobileNetV2_extrator[0][1]       \n",
      "                                                                 MobileNetV2_extrator[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "SSH_0 (SSH)                     (None, None, None, 6 34944       FPN[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "SSH_1 (SSH)                     (None, None, None, 6 34944       FPN[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "SSH_2 (SSH)                     (None, None, None, 6 34944       FPN[0][2]                        \n",
      "__________________________________________________________________________________________________\n",
      "ClassHead_0 (ClassHead)         (None, None, 2)      260         SSH_0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ClassHead_1 (ClassHead)         (None, None, 2)      260         SSH_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ClassHead_2 (ClassHead)         (None, None, 2)      260         SSH_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BboxHead_0 (BboxHead)           (None, None, 4)      520         SSH_0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BboxHead_1 (BboxHead)           (None, None, 4)      520         SSH_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BboxHead_2 (BboxHead)           (None, None, 4)      520         SSH_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_1 (TFOpLambda)        (None, None, 2)      0           ClassHead_0[0][0]                \n",
      "                                                                 ClassHead_1[0][0]                \n",
      "                                                                 ClassHead_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, None, 4)      0           BboxHead_0[0][0]                 \n",
      "                                                                 BboxHead_1[0][0]                 \n",
      "                                                                 BboxHead_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, None, 2)      0           tf.concat_1[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,811,236\n",
      "Trainable params: 1,782,948\n",
      "Non-trainable params: 28,288\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf944c1-c673-4e26-8e68-3352bb25e31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Convert Keras model to ConcreteFunction\n",
    "# full_model = tf.function(lambda x: model(x))\n",
    "# full_model = full_model.get_concrete_function(\n",
    "#     tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ff2cb33-273b-4a5d-9325-d3fd8431faac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # tf.compat.v1.global_variables_initializer().run()\n",
    "# # tf.initializers.global_variables_initializer().run()\n",
    "\n",
    "# frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "# gph = frozen_func.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "208b9bf6-ebab-4f2b-82e4-d21049d2b98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# type(gph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19166932-8005-440f-896c-18db36c999dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gph_li = list(gph.node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c755703f-412b-4b5d-b75d-9e7af88f6dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gph_li[587]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61b684a1-1e1b-4caf-958b-97546be16382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import coremltools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f9a8b89-8d55-4dfe-b37f-be695e2294ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3.0\n",
      "x[0] :- (None, None, None, 192)\n",
      "x[1] :- (None, None, None, 576)\n",
      "x[2] :- (None, None, None, 960)\n",
      "Running output1 conv layer\n",
      "Output of output1 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output2 conv layer\n",
      "Output of output2 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output3 conv layer\n",
      "Output of output3 conv :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output3 \n",
      "\n",
      "None None\n",
      "Resized output3 to :- Tensor(\"RetinaFaceModel/FPN/Shape_2:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output2 to resized output3 \n",
      "\n",
      "Output2 shape :- Tensor(\"RetinaFaceModel/FPN/Shape_3:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "passing output2 to merge2 conv \n",
      "\n",
      "Output of output2 :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output2 w.r.t. output1 \n",
      "\n",
      "Resized output2 to :- Tensor(\"RetinaFaceModel/FPN/Shape_6:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output1 to resized output2 \n",
      "\n",
      "Output1 shape :- Tensor(\"RetinaFaceModel/FPN/Shape_7:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Running merge1 conv on output1 \n",
      "\n",
      "Shape of output1 :- (None, None, None, 64) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 06:07:44.646814: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-07-13 06:07:44.646973: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2023-07-13 06:07:44.660879: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.009ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2023-07-13 06:07:45.481740: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-07-13 06:07:45.481880: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2023-07-13 06:07:45.597411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 721 nodes (-353), 1464 edges (-353), time = 52.295ms.\n",
      "  dependency_optimizer: Graph size after: 712 nodes (-9), 749 edges (-715), time = 9.135ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.77ms.\n",
      "  constant_folding: Graph size after: 712 nodes (0), 749 edges (0), time = 16.308ms.\n",
      "  dependency_optimizer: Graph size after: 712 nodes (0), 749 edges (0), time = 7.075ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.689ms.\n",
      "\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 17.19 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops:  84%|████████▎ | 595/712 [00:00<00:00, 1614.01 ops/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'concat' object has no attribute 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 35\u001b[0m\n\u001b[1;32m     16\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mShape(shape\u001b[38;5;241m=\u001b[39m(ct\u001b[38;5;241m.\u001b[39mRangeDim(lower_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, upper_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     17\u001b[0m                     ct\u001b[38;5;241m.\u001b[39mRangeDim(lower_bound\u001b[38;5;241m=\u001b[39mheight, upper_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     18\u001b[0m                     ct\u001b[38;5;241m.\u001b[39mRangeDim(lower_bound\u001b[38;5;241m=\u001b[39mwidth, upper_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     19\u001b[0m                     \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#                     1024,\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#                     3))\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m c_model \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# pass_pipeline=pipeline,\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m                     \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/_converters_entry.py:492\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m specification_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     specification_version \u001b[38;5;241m=\u001b[39m _set_default_specification_version(exact_target)\n\u001b[0;32m--> 492\u001b[0m mlmodel \u001b[38;5;241m=\u001b[39m \u001b[43mmil_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs_as_tensor_or_image_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# None or list[ct.ImageType/ct.TensorType]\u001b[39;49;00m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_model_load\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_model_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecification_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecification_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exact_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mlmodel  \u001b[38;5;66;03m# Returns the MIL program\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/converter.py:188\u001b[0m, in \u001b[0;36mmil_convert\u001b[0;34m(model, convert_from, convert_to, compute_units, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;129m@_profile\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmil_convert\u001b[39m(\n\u001b[1;32m    151\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    156\u001b[0m ):\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Convert model from a specified frontend `convert_from` to a specified\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    converter backend `convert_to`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m        See `coremltools.converters.convert`\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mil_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConverterRegistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMLModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/converter.py:212\u001b[0m, in \u001b[0;36m_mil_convert\u001b[0;34m(model, convert_from, convert_to, registry, modelClass, compute_units, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     weights_dir \u001b[38;5;241m=\u001b[39m _tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory()\n\u001b[1;32m    210\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weights_dir\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 212\u001b[0m proto, mil_program \u001b[38;5;241m=\u001b[39m \u001b[43mmil_convert_to_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m _reset_conversion_state()\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/converter.py:285\u001b[0m, in \u001b[0;36mmil_convert_to_proto\u001b[0;34m(model, convert_from, convert_to, converter_registry, main_pipeline, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m frontend_pipeline, backend_pipeline \u001b[38;5;241m=\u001b[39m _construct_other_pipelines(\n\u001b[1;32m    281\u001b[0m     main_pipeline, convert_from, convert_to\n\u001b[1;32m    282\u001b[0m )\n\u001b[1;32m    284\u001b[0m frontend_converter \u001b[38;5;241m=\u001b[39m frontend_converter_type()\n\u001b[0;32m--> 285\u001b[0m prog \u001b[38;5;241m=\u001b[39m \u001b[43mfrontend_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m PipelineManager\u001b[38;5;241m.\u001b[39mapply_pipeline(prog, frontend_pipeline)\n\u001b[1;32m    288\u001b[0m PipelineManager\u001b[38;5;241m.\u001b[39mapply_pipeline(prog, main_pipeline)\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/converter.py:98\u001b[0m, in \u001b[0;36mTensorFlow2Frontend.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TF2Loader\n\u001b[1;32m     97\u001b[0m tf2_loader \u001b[38;5;241m=\u001b[39m TF2Loader(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf2_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/load.py:82\u001b[0m, in \u001b[0;36mTFLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m     dot_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_ssa\u001b[38;5;241m.\u001b[39mget_dot_string(\n\u001b[1;32m     76\u001b[0m         annotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name_and_op_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, highlight_debug_nodes\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     78\u001b[0m     graphviz\u001b[38;5;241m.\u001b[39mSource(dot_string)\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m     79\u001b[0m         filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/ssa_before_tf_passes\u001b[39m\u001b[38;5;124m\"\u001b[39m, cleanup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m program \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_program_from_tf_ssa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprogram:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(program))\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m program\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow2/load.py:210\u001b[0m, in \u001b[0;36mTF2Loader._program_from_tf_ssa\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_tf_ssa_passes()\n\u001b[1;32m    204\u001b[0m converter \u001b[38;5;241m=\u001b[39m TF2Converter(\n\u001b[1;32m    205\u001b[0m     tfssa\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_ssa,\n\u001b[1;32m    206\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    207\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    208\u001b[0m     opset_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecification_version\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    209\u001b[0m )\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/converter.py:465\u001b[0m, in \u001b[0;36mTFConverter.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_stack[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39madd_graph(g_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfssa\u001b[38;5;241m.\u001b[39mfunctions[g_name]\u001b[38;5;241m.\u001b[39mgraph)\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_main_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prog\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/converter.py:389\u001b[0m, in \u001b[0;36mTFConverter.convert_main_graph\u001b[0;34m(self, prog, graph)\u001b[0m\n\u001b[1;32m    387\u001b[0m         input_var \u001b[38;5;241m=\u001b[39m mb\u001b[38;5;241m.\u001b[39mcast(x\u001b[38;5;241m=\u001b[39minput_var, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp32\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39madd(name, input_var)\n\u001b[0;32m--> 389\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m ssa_func\u001b[38;5;241m.\u001b[39mset_outputs(outputs)\n\u001b[1;32m    391\u001b[0m prog\u001b[38;5;241m.\u001b[39madd_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m, ssa_func)\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/convert_utils.py:191\u001b[0m, in \u001b[0;36mconvert_graph\u001b[0;34m(context, graph, outputs)\u001b[0m\n\u001b[1;32m    187\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion for TF op \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    188\u001b[0m         node\u001b[38;5;241m.\u001b[39mop, node\u001b[38;5;241m.\u001b[39moriginal_node\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[0;32m--> 191\u001b[0m \u001b[43madd_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# set_global / get_global / NoOp has no direct consumer / outputs\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     x \u001b[38;5;241m=\u001b[39m context[node\u001b[38;5;241m.\u001b[39mname]\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/ops.py:2584\u001b[0m, in \u001b[0;36mResizeNearestNeighbor\u001b[0;34m(context, node)\u001b[0m\n\u001b[1;32m   2580\u001b[0m Hout, Wout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context[node\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2582\u001b[0m     \u001b[38;5;66;03m# for the dynamic input shape case,\u001b[39;00m\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;66;03m# context[node.inputs[1]] is a mul(x=input_shape, y=scaling_factor) op.\u001b[39;00m\n\u001b[0;32m-> 2584\u001b[0m     scaling_factor_h \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241m.\u001b[39mval[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2585\u001b[0m     scaling_factor_w \u001b[38;5;241m=\u001b[39m context[node\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mval[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'concat' object has no attribute 'y'"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "\n",
    "# # import coremltools as ct\n",
    "# pipeline = ct.PassPipeline()\n",
    "# pipeline.remove_passes({\"common::fuse_conv_batchnorm\"})\n",
    "\n",
    "\n",
    "print(ct.__version__)\n",
    "\n",
    "input_name = model.inputs[0].name\n",
    "\n",
    "height = 256\n",
    "width = 256\n",
    "\n",
    "input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=height, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=width, upper_bound=-1),\n",
    "                    3))\n",
    "\n",
    "# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\n",
    "\n",
    "\n",
    "input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "                    800,\n",
    "                    800,\n",
    "                    3))\n",
    "\n",
    "input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "                    1024,\n",
    "                    1024,\n",
    "                    3))\n",
    "\n",
    "\n",
    "c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], \n",
    "                     # pass_pipeline=pipeline,\n",
    "                     source='tensorflow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df0b02b4-4c2a-4d4d-8b4b-4501fe3d9de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_model.save(\"./retinaface-1024.mlmodel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdd13435-52bf-4be7-94d8-f98f2a0ee052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_model.save(\"./retinaface-800.mlmodel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b6e04-632d-4514-9199-b1162e442e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from coremltools.models.neural_network import flexible_shape_utils\n",
    "\n",
    "spec = ct.utils.load_spec('./retinaface-800.mlmodel')\n",
    "\n",
    "# to update the shapes of the first input\n",
    "input_name = spec.description.input[0].name\n",
    "\n",
    "\n",
    "# Range shapes and multi array inputs\n",
    "# -1 in upper bound represents \"infinity\" \n",
    "flexible_shape_utils.set_multiarray_ndshape_range(spec, \n",
    "                                 feature_name=input_name, \n",
    "                                 lower_bounds=[1,800,800,3], \n",
    "                                 upper_bounds=[-1,-1,-1,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d5f61fcf-8a61-4250-bfd8-9228b92e7549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod = ct.models.MLModel(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0e82a495-d664-4504-ace8-e56c404f28ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod.save(\"retinaface_800-tf2-dynamic_shape.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f62d6-ead1-4ca6-be84-3b0319556526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7fe3b871-b6e9-4e45-ab92-46d5a826eab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.keras.backend.set_learning_phase(0)\n",
    "# model_pth = \"./temp_with_weights_retinaface_800_ckpnt78.h5\"\n",
    "# model = tf.keras.models.load_model(model_pth,\n",
    "#                                    custom_objects={\"FPN\": FPN, \"SSH\":SSH, \"ClassHead\": ClassHead, \"BboxHead\": BboxHead}, compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b44c2adb-713e-475f-bd32-0c4a67fc6a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b51b0-ef18-469f-8558-02bb89c7a4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8347b-22f4-447c-a51d-a8d1c6a9315b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import ImageFont\n",
    "\n",
    "# # font = ImageFont.truetype(\"arial.ttf\", 32)  # using comic sans is strictly prohibited!\n",
    "# visualkeras.layered_view(model, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d419b01-c518-4271-bb5c-cf2b81d36ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d49b86a-8138-4982-8e2b-b07126312303",
   "metadata": {},
   "source": [
    "# try converting to dynamic shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ed772-5548-4061-8170-c2fd06cc7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "\n",
    "# import coremltools as ct\n",
    "pipeline = ct.PassPipeline()\n",
    "pipeline.remove_passes({\"common::fuse_conv_batchnorm\"})\n",
    "\n",
    "\n",
    "print(ct.__version__)\n",
    "\n",
    "input_name = model.inputs[0].name\n",
    "\n",
    "height = 256\n",
    "width = 256\n",
    "\n",
    "input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=height, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=width, upper_bound=-1),\n",
    "                    3))\n",
    "\n",
    "# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\n",
    "\n",
    "\n",
    "# input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "#                     800,\n",
    "#                     800,\n",
    "#                     3))\n",
    "\n",
    "# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\n",
    "c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], source='tensorflow')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31a415-3eb3-482f-8422-c9f0b8cab181",
   "metadata": {},
   "source": [
    "# divide model into parts and try to convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8267985-5448-47bf-bfb3-682422dea76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extractor = MobileNetV2(\n",
    "#                 input_shape=None, include_top=False, weights=\"imagenet\")\n",
    "\n",
    "# import coremltools as ct\n",
    "\n",
    "\n",
    "# # import coremltools as ct\n",
    "# pipeline = ct.PassPipeline()\n",
    "# pipeline.remove_passes({\"common::fuse_conv_batchnorm\"})\n",
    "\n",
    "\n",
    "# print(ct.__version__)\n",
    "\n",
    "# input_name = extractor.inputs[0].name\n",
    "\n",
    "# height = 256\n",
    "# width = 256\n",
    "\n",
    "# input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "#                     ct.RangeDim(lower_bound=height, upper_bound=-1),\n",
    "#                     ct.RangeDim(lower_bound=width, upper_bound=-1),\n",
    "#                     3))\n",
    "\n",
    "# # c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\n",
    "\n",
    "\n",
    "# # input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "# #                     800,\n",
    "# #                     800,\n",
    "# #                     3))\n",
    "\n",
    "# # c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\n",
    "# c_model = ct.convert(extractor, inputs=[ct.TensorType(shape=input_shape, name=input_name)], source='tensorflow')\n",
    "\n",
    "input_size = None\n",
    "\n",
    "x = inputs = Input([input_size, input_size, 3], name='input_image')\n",
    "preprocess = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "extractor = MobileNetV2(\n",
    "    input_shape= None, include_top=False, weights=\"imagenet\")\n",
    "pick_layer1 = 54  # [80, 80, 32]\n",
    "pick_layer2 = 116  # [40, 40, 96]\n",
    "pick_layer3 = 143  # [20, 20, 160]\n",
    "preprocess = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "extractor = Model(extractor.input,\n",
    "                     (extractor.layers[pick_layer1].output,\n",
    "                      extractor.layers[pick_layer2].output,\n",
    "                      extractor.layers[pick_layer3].output),\n",
    "                     name=\"mobilenet\" + '_extrator')(preprocess(x))\n",
    "\n",
    "\n",
    "\n",
    "extractor1 = Model(x, extractor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff991cd2-3981-4f46-ae8f-c6daaf42f053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('block_6_expand', 'block_13_expand', 'block_16_expand')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "extractor = MobileNetV2(\n",
    "    input_shape=None, include_top=False, weights=\"imagenet\")\n",
    "pick_layer1 = 54  # [80, 80, 32]\n",
    "pick_layer2 = 116  # [40, 40, 96]\n",
    "pick_layer3 = 143  # [20, 20, 160]\n",
    "preprocess = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "(extractor.layers[54].name,\n",
    "extractor.layers[pick_layer2].name,\n",
    "extractor.layers[pick_layer3].name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29538055-c430-4278-b4ca-a00e2908356a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9dd82c45b0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9dd826b100>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9dd821cd90>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(extractor.layers[54],\n",
    "extractor.layers[pick_layer2],\n",
    "extractor.layers[pick_layer3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3b5e1fa-a678-4ff7-bdda-bad9e2573b43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.functional.Functional object at 0x7f9fd8173490> <tensorflow.python.keras.engine.functional.Functional object at 0x7f9df4162280>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnt = 0\n",
    "\n",
    "for layers_og, layers in zip(model.layers, extractor1.layers):\n",
    "    if cnt == 3:\n",
    "        print(layers_og, layers)\n",
    "        for ly_og, ly in zip(layers_og.layers, layers.layers):\n",
    "            ly.set_weights(ly_og.get_weights())\n",
    "        break\n",
    "    else:\n",
    "        cnt += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e4473e9-d309-4dd1-b937-49bb9db0fb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import coremltools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e782e1e0-5d88-4903-8765-f1af007b169f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 06:08:51.425101: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-07-13 06:08:51.425223: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2023-07-13 06:08:51.433838: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.007ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2023-07-13 06:08:51.921459: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-07-13 06:08:51.921608: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2023-07-13 06:08:51.989304: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 396 nodes (-241), 1128 edges (-241), time = 30.946ms.\n",
      "  dependency_optimizer: Graph size after: 396 nodes (0), 405 edges (-723), time = 5.301ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.411ms.\n",
      "  constant_folding: Graph size after: 396 nodes (0), 405 edges (0), time = 8.941ms.\n",
      "  dependency_optimizer: Graph size after: 396 nodes (0), 405 edges (0), time = 3.637ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.377ms.\n",
      "\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 17.45 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 396/396 [00:00<00:00, 1297.92 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 242.67 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 56/56 [00:01<00:00, 37.24 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 8/8 [00:00<00:00, 786.35 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 475/475 [00:00<00:00, 2455.40 ops/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_name = extractor1.inputs[0].name\n",
    "\n",
    "height = 256\n",
    "width = 256\n",
    "\n",
    "input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=height, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=width, upper_bound=-1),\n",
    "                    3))\n",
    "\n",
    "# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\n",
    "\n",
    "\n",
    "# input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "#                     800,\n",
    "#                     800,\n",
    "#                     3))\n",
    "\n",
    "# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\n",
    "c_model = ct.convert(extractor1, inputs=[ct.TensorType(shape=input_shape, name=input_name)], source='tensorflow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb6e1384-b21b-443c-9c4f-3ce48c2d68c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8bff493-3b57-4975-8531-5b5b4532ec42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(None, None, None, 192) dtype=float32 (created by layer 'mobilenet_extrator')>,\n",
       " <KerasTensor: shape=(None, None, None, 576) dtype=float32 (created by layer 'mobilenet_extrator')>,\n",
       " <KerasTensor: shape=(None, None, None, 960) dtype=float32 (created by layer 'mobilenet_extrator')>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor1.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "672c59ea-4094-4a51-8ad3-98f3dc7df9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(None, None, None, 192) dtype=float32 (created by layer 'MobileNetV2_extrator')>,\n",
       " <KerasTensor: shape=(None, None, None, 576) dtype=float32 (created by layer 'MobileNetV2_extrator')>,\n",
       " <KerasTensor: shape=(None, None, None, 960) dtype=float32 (created by layer 'MobileNetV2_extrator')>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[4].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af4badb8-6834-4c6e-943a-e052338e2f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, None, None, None, 192) dtype=float32 (created by layer 'input_4')>,\n",
       " <KerasTensor: shape=(None, None, None, None, 576) dtype=float32 (created by layer 'input_5')>,\n",
       " <KerasTensor: shape=(None, None, None, None, 960) dtype=float32 (created by layer 'input_6')>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input(shape=[[None, None, None, 192], (None, None, None, 576), (None, None, None, 960)])\n",
    "\n",
    "[Input(shape= (None, None, None, 192)), Input(shape= (None, None, None, 576)), Input(shape= (None, None, None, 960))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60211883-564f-4b6f-9a77-62fb02467a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, None, None, 192) dtype=float32 (created by layer 'mobilenet_extrator')>,\n",
       " <KerasTensor: shape=(None, None, None, 576) dtype=float32 (created by layer 'mobilenet_extrator')>,\n",
       " <KerasTensor: shape=(None, None, None, 960) dtype=float32 (created by layer 'mobilenet_extrator')>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor1.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "503fc8e7-5884-4427-9069-b0cf47780f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 32]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['min_sizes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7342a98c-174a-4e05-9931-f333ff5bc93e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "x[0] :- (None, None, None, 192)\n",
      "x[1] :- (None, None, None, 576)\n",
      "x[2] :- (None, None, None, 960)\n",
      "Running output1 conv layer\n",
      "Output of output1 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output2 conv layer\n",
      "Output of output2 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output3 conv layer\n",
      "Output of output3 conv :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output3 \n",
      "\n",
      "None None\n",
      "Resized output3 to :- Tensor(\"FPN/Shape_2:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output2 to resized output3 \n",
      "\n",
      "Output2 shape :- Tensor(\"FPN/Shape_3:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "passing output2 to merge2 conv \n",
      "\n",
      "Output of output2 :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output2 w.r.t. output1 \n",
      "\n",
      "Resized output2 to :- Tensor(\"FPN/Shape_6:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output1 to resized output2 \n",
      "\n",
      "Output1 shape :- Tensor(\"FPN/Shape_7:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Running merge1 conv on output1 \n",
      "\n",
      "Shape of output1 :- (None, None, None, 64) \n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FPN (FPN)                       ((None, None, None,  185600      input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 185,600\n",
      "Trainable params: 184,960\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wd = cfg['weights_decay']\n",
    "out_ch = cfg['out_channel']\n",
    "print(out_ch)\n",
    "\n",
    "\n",
    "x = (Input(shape= (None, None, 192)), Input(shape= (None, None, 576)), Input(shape= (None, None, 960)))\n",
    "\n",
    "# x = (Input(shape= (100, 100, 192)), Input(shape= (50, 50, 576)), Input(shape= (25, 25, 960)))\n",
    "\n",
    "\n",
    "# name = \"CU\"\n",
    "# output1 = ConvUnit(f=out_ch, k=1, s=1, wd=wd, act=\"relu\", name=name+str(1))\n",
    "# output1(x[0])\n",
    "\n",
    "fpn = FPN(out_ch=out_ch, wd=wd)\n",
    "# fpn(x)\n",
    "\n",
    "fpn_mdl = Model(inputs=x, outputs=fpn(x))\n",
    "fpn_mdl.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbae24d6-db43-49d6-b273-ee67b3b55c80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_9'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpn_mdl.inputs[2].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55034b9c-01f3-438e-ae5a-55c260694edc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0] :- (None, None, None, 192)\n",
      "x[1] :- (None, None, None, 576)\n",
      "x[2] :- (None, None, None, 960)\n",
      "Running output1 conv layer\n",
      "Output of output1 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output2 conv layer\n",
      "Output of output2 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output3 conv layer\n",
      "Output of output3 conv :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output3 \n",
      "\n",
      "None None\n",
      "Resized output3 to :- Tensor(\"model_1/FPN/Shape_2:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output2 to resized output3 \n",
      "\n",
      "Output2 shape :- Tensor(\"model_1/FPN/Shape_3:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "passing output2 to merge2 conv \n",
      "\n",
      "Output of output2 :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output2 w.r.t. output1 \n",
      "\n",
      "Resized output2 to :- Tensor(\"model_1/FPN/Shape_6:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output1 to resized output2 \n",
      "\n",
      "Output1 shape :- Tensor(\"model_1/FPN/Shape_7:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Running merge1 conv on output1 \n",
      "\n",
      "Shape of output1 :- (None, None, None, 64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Keras model to ConcreteFunction\n",
    "full_model = tf.function(lambda x: fpn_mdl(x))\n",
    "full_model = full_model.get_concrete_function(\n",
    "    [\n",
    "        tf.TensorSpec(fpn_mdl.inputs[0].shape, fpn_mdl.inputs[0].dtype), tf.TensorSpec(fpn_mdl.inputs[1].shape, fpn_mdl.inputs[1].dtype), \n",
    "        tf.TensorSpec(fpn_mdl.inputs[2].shape, fpn_mdl.inputs[2].dtype)\n",
    "                   ])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e304f86b-3c8e-4198-a2f0-e69d78be2ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 06:09:12.939260: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-07-13 06:09:12.939380: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2023-07-13 06:09:12.942277: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.01ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.global_variables_initializer().run()\n",
    "# tf.initializers.global_variables_initializer().run()\n",
    "\n",
    "frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "gph = frozen_func.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c662ec9a-9f07-40f0-9c53-7ccac11e61f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gph_nd = list(gph.node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcfcf235-d422-429f-89be-f3a72ca874d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"model_1/FPN/FPN1/bn/ReadVariableOp_1\"\n",
       "op: \"Identity\"\n",
       "input: \"model_1/FPN/FPN1/bn/ReadVariableOp_1/resource\"\n",
       "attr {\n",
       "  key: \"T\"\n",
       "  value {\n",
       "    type: DT_FLOAT\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gph_nd[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7103551-0fa8-4115-b75c-d47045299817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0] :- (None, None, None, 192)\n",
      "x[1] :- (None, None, None, 576)\n",
      "x[2] :- (None, None, None, 960)\n",
      "Running output1 conv layer\n",
      "Output of output1 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output2 conv layer\n",
      "Output of output2 conv :- (None, None, None, 64) \n",
      "\n",
      "Running output3 conv layer\n",
      "Output of output3 conv :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output3 \n",
      "\n",
      "None None\n",
      "Resized output3 to :- Tensor(\"model_1/FPN/Shape_2:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output2 to resized output3 \n",
      "\n",
      "Output2 shape :- Tensor(\"model_1/FPN/Shape_3:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "passing output2 to merge2 conv \n",
      "\n",
      "Output of output2 :- (None, None, None, 64) \n",
      "\n",
      "Running resize of output2 w.r.t. output1 \n",
      "\n",
      "Resized output2 to :- Tensor(\"model_1/FPN/Shape_6:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Adding output1 to resized output2 \n",
      "\n",
      "Output1 shape :- Tensor(\"model_1/FPN/Shape_7:0\", shape=(4,), dtype=int32) (None, None, None, 64)\n",
      "\n",
      "Running merge1 conv on output1 \n",
      "\n",
      "Shape of output1 :- (None, None, None, 64) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 06:09:16.673913: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-07-13 06:09:16.674050: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2023-07-13 06:09:16.676813: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.007ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2023-07-13 06:09:16.752773: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-07-13 06:09:16.752891: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2023-07-13 06:09:16.763719: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 72 nodes (-25), 150 edges (-25), time = 3.86ms.\n",
      "  dependency_optimizer: Graph size after: 72 nodes (0), 75 edges (-75), time = 0.732ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.073ms.\n",
      "  constant_folding: Graph size after: 72 nodes (0), 75 edges (0), time = 1.369ms.\n",
      "  dependency_optimizer: Graph size after: 72 nodes (0), 75 edges (0), time = 0.586ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.06ms.\n",
      "\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 31.10 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops:  83%|████████▎ | 60/72 [00:00<00:00, 2414.36 ops/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'concat' object has no attribute 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 36\u001b[0m\n\u001b[1;32m     19\u001b[0m input_shape3 \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mShape(shape\u001b[38;5;241m=\u001b[39m(ct\u001b[38;5;241m.\u001b[39mRangeDim(lower_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, upper_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     20\u001b[0m                     ct\u001b[38;5;241m.\u001b[39mRangeDim(lower_bound\u001b[38;5;241m=\u001b[39mheight, upper_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     21\u001b[0m                     ct\u001b[38;5;241m.\u001b[39mRangeDim(lower_bound\u001b[38;5;241m=\u001b[39mwidth, upper_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     22\u001b[0m                     \u001b[38;5;241m960\u001b[39m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m fpn_model \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpn_mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# # out = fpn\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# features = [SSH(out_ch=out_ch, wd=wd, name=f'SSH_{i}')(f)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# model_after_extractor = Model(fpn, out)\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/_converters_entry.py:492\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m specification_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     specification_version \u001b[38;5;241m=\u001b[39m _set_default_specification_version(exact_target)\n\u001b[0;32m--> 492\u001b[0m mlmodel \u001b[38;5;241m=\u001b[39m \u001b[43mmil_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs_as_tensor_or_image_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# None or list[ct.ImageType/ct.TensorType]\u001b[39;49;00m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_model_load\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_model_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecification_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecification_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exact_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mlmodel  \u001b[38;5;66;03m# Returns the MIL program\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/converter.py:188\u001b[0m, in \u001b[0;36mmil_convert\u001b[0;34m(model, convert_from, convert_to, compute_units, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;129m@_profile\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmil_convert\u001b[39m(\n\u001b[1;32m    151\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    156\u001b[0m ):\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Convert model from a specified frontend `convert_from` to a specified\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    converter backend `convert_to`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m        See `coremltools.converters.convert`\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mil_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConverterRegistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMLModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/converter.py:212\u001b[0m, in \u001b[0;36m_mil_convert\u001b[0;34m(model, convert_from, convert_to, registry, modelClass, compute_units, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     weights_dir \u001b[38;5;241m=\u001b[39m _tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory()\n\u001b[1;32m    210\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weights_dir\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 212\u001b[0m proto, mil_program \u001b[38;5;241m=\u001b[39m \u001b[43mmil_convert_to_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m _reset_conversion_state()\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/converter.py:285\u001b[0m, in \u001b[0;36mmil_convert_to_proto\u001b[0;34m(model, convert_from, convert_to, converter_registry, main_pipeline, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m frontend_pipeline, backend_pipeline \u001b[38;5;241m=\u001b[39m _construct_other_pipelines(\n\u001b[1;32m    281\u001b[0m     main_pipeline, convert_from, convert_to\n\u001b[1;32m    282\u001b[0m )\n\u001b[1;32m    284\u001b[0m frontend_converter \u001b[38;5;241m=\u001b[39m frontend_converter_type()\n\u001b[0;32m--> 285\u001b[0m prog \u001b[38;5;241m=\u001b[39m \u001b[43mfrontend_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m PipelineManager\u001b[38;5;241m.\u001b[39mapply_pipeline(prog, frontend_pipeline)\n\u001b[1;32m    288\u001b[0m PipelineManager\u001b[38;5;241m.\u001b[39mapply_pipeline(prog, main_pipeline)\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/converter.py:98\u001b[0m, in \u001b[0;36mTensorFlow2Frontend.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrontend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TF2Loader\n\u001b[1;32m     97\u001b[0m tf2_loader \u001b[38;5;241m=\u001b[39m TF2Loader(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf2_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/load.py:82\u001b[0m, in \u001b[0;36mTFLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m     dot_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_ssa\u001b[38;5;241m.\u001b[39mget_dot_string(\n\u001b[1;32m     76\u001b[0m         annotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name_and_op_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, highlight_debug_nodes\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     78\u001b[0m     graphviz\u001b[38;5;241m.\u001b[39mSource(dot_string)\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m     79\u001b[0m         filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/ssa_before_tf_passes\u001b[39m\u001b[38;5;124m\"\u001b[39m, cleanup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m program \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_program_from_tf_ssa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprogram:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(program))\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m program\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow2/load.py:210\u001b[0m, in \u001b[0;36mTF2Loader._program_from_tf_ssa\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_tf_ssa_passes()\n\u001b[1;32m    204\u001b[0m converter \u001b[38;5;241m=\u001b[39m TF2Converter(\n\u001b[1;32m    205\u001b[0m     tfssa\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_ssa,\n\u001b[1;32m    206\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    207\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    208\u001b[0m     opset_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecification_version\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    209\u001b[0m )\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/converter.py:465\u001b[0m, in \u001b[0;36mTFConverter.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_stack[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39madd_graph(g_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfssa\u001b[38;5;241m.\u001b[39mfunctions[g_name]\u001b[38;5;241m.\u001b[39mgraph)\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_main_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prog\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/converter.py:389\u001b[0m, in \u001b[0;36mTFConverter.convert_main_graph\u001b[0;34m(self, prog, graph)\u001b[0m\n\u001b[1;32m    387\u001b[0m         input_var \u001b[38;5;241m=\u001b[39m mb\u001b[38;5;241m.\u001b[39mcast(x\u001b[38;5;241m=\u001b[39minput_var, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp32\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39madd(name, input_var)\n\u001b[0;32m--> 389\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m ssa_func\u001b[38;5;241m.\u001b[39mset_outputs(outputs)\n\u001b[1;32m    391\u001b[0m prog\u001b[38;5;241m.\u001b[39madd_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m, ssa_func)\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/convert_utils.py:191\u001b[0m, in \u001b[0;36mconvert_graph\u001b[0;34m(context, graph, outputs)\u001b[0m\n\u001b[1;32m    187\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion for TF op \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    188\u001b[0m         node\u001b[38;5;241m.\u001b[39mop, node\u001b[38;5;241m.\u001b[39moriginal_node\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[0;32m--> 191\u001b[0m \u001b[43madd_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# set_global / get_global / NoOp has no direct consumer / outputs\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     x \u001b[38;5;241m=\u001b[39m context[node\u001b[38;5;241m.\u001b[39mname]\n",
      "File \u001b[0;32m~/SageMaker/envs/coreml_env/lib64/python3.8/site-packages/coremltools/converters/mil/frontend/tensorflow/ops.py:2584\u001b[0m, in \u001b[0;36mResizeNearestNeighbor\u001b[0;34m(context, node)\u001b[0m\n\u001b[1;32m   2580\u001b[0m Hout, Wout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context[node\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2582\u001b[0m     \u001b[38;5;66;03m# for the dynamic input shape case,\u001b[39;00m\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;66;03m# context[node.inputs[1]] is a mul(x=input_shape, y=scaling_factor) op.\u001b[39;00m\n\u001b[0;32m-> 2584\u001b[0m     scaling_factor_h \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241m.\u001b[39mval[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2585\u001b[0m     scaling_factor_w \u001b[38;5;241m=\u001b[39m context[node\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mval[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'concat' object has no attribute 'y'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_name = fpn_mdl.inputs[0].name\n",
    "input_name1 = fpn_mdl.inputs[1].name\n",
    "input_name2 = fpn_mdl.inputs[2].name\n",
    "\n",
    "height = 200\n",
    "width = 200\n",
    "\n",
    "\n",
    "input_shape1 = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=height, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=width, upper_bound=-1),\n",
    "                    192))\n",
    "\n",
    "input_shape2 = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=height, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=width, upper_bound=-1),\n",
    "                    576))\n",
    "\n",
    "input_shape3 = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=height, upper_bound=-1),\n",
    "                    ct.RangeDim(lower_bound=width, upper_bound=-1),\n",
    "                    960))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\n",
    "\n",
    "\n",
    "# input_shape = ct.Shape(shape=(ct.RangeDim(lower_bound=1, upper_bound=-1),\n",
    "#                     800,\n",
    "#                     800,\n",
    "#                     3))\n",
    "\n",
    "# c_model = ct.convert(model, inputs=[ct.TensorType(shape=input_shape, name=input_name)], pass_pipeline=pipeline, source='tensorflow')\n",
    "fpn_model = ct.convert(fpn_mdl, inputs=[ct.TensorType(shape=input_shape1, name=input_name), ct.TensorType(shape=input_shape2, name=input_name1), ct.TensorType(shape=input_shape3, name=input_name2)], source='tensorflow')\n",
    "\n",
    "\n",
    "# # out = fpn\n",
    "\n",
    "# features = [SSH(out_ch=out_ch, wd=wd, name=f'SSH_{i}')(f)\n",
    "#             for i, f in enumerate(fpn)]\n",
    "\n",
    "# bbox_regressions = tf.concat(\n",
    "#     [BboxHead(num_anchor, wd=wd, name=f'BboxHead_{i}')(f)\n",
    "#      for i, f in enumerate(features)], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# # landm_regressions = tf.concat(\n",
    "# #     [LandmarkHead(num_anchor, wd=wd, name=f'LandmarkHead_{i}')(f)\n",
    "# #      for i, f in enumerate(features)], axis=1)\n",
    "# classifications = tf.concat(\n",
    "#     [ClassHead(num_anchor, wd=wd, name=f'ClassHead_{i}')(f)\n",
    "#      for i, f in enumerate(features)], axis=1)\n",
    "\n",
    "# classifications = tf.keras.layers.Softmax(axis=-1)(classifications)\n",
    "\n",
    "\n",
    "# bbox_reg = [BboxHead(num_anchor, wd=wd, name=f'BboxHead_{i}')(f)\n",
    "#      for i, f in enumerate(features)]\n",
    "\n",
    "\n",
    "# clf = [ClassHead(num_anchor, wd=wd, name=f'ClassHead_{i}')(f)\n",
    "#      for i, f in enumerate(features)]\n",
    "\n",
    "# out = (bbox_regressions, \n",
    "#        # landm_regressions, \n",
    "#        classifications)\n",
    "\n",
    "# model_after_extractor = Model(fpn, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a088c3-294e-488e-8d81-a0ea106db5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extractor1.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3a71b-c9f3-4f32-809a-6d3d444cba4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fpn(extractor1.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "40a94bcc-b5c5-4816-8a59-42a74975f6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extractor1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f8823-86ef-4abe-823b-44567dcff907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc4748-9c80-478f-8a78-eb6b77b4ae43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a5361-b25e-4e6b-b3cf-3f078cce9781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669c0c7-3657-4e2d-a71b-a602c26d88db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreml_env",
   "language": "python",
   "name": "coreml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
